{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d65053c-2ef7-44b0-a4ec-fa3875d50e2e",
   "metadata": {},
   "source": [
    "# **Regular Languages**\n",
    "\n",
    "This Jupyter Notebook is intended to showcase the use of the `pyfoma` finite state library. Basic information about pyfoma can be found in its [documentation](https://github.com/mhulden/pyfoma/blob/main/README.md) and the description of its [regular expression metalanguage](https://github.com/mhulden/pyfoma/blob/main/docs/RegularExpressionCompiler.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "644e569c-f51f-4212-8232-39ff0090021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pyfoma ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b04573-e0a9-403a-85a7-6c7417b541d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyfoma\n",
    "from pyfoma import FST\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9191d1c7-ca51-4860-a0c2-18ca1b06f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "try:\n",
    "    get_ipython()\n",
    "    import ipytest\n",
    "    ipytest.autoconfig()\n",
    "    def init_test():\n",
    "        ipytest.clean()\n",
    "    def run_test():\n",
    "        ipytest.run()\n",
    "except NameError:\n",
    "    def init_test():\n",
    "        pass\n",
    "    def run_test():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7674086e-6933-4151-9a54-f534239ff35f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dates\n",
    "\n",
    "The following regular expression only accepts dates in the form `MM/DD/YYYY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f16b9f8b-f41b-4609-97e8-1d796572c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = FST.re(r\"([1-9]|0[1-9]|1[0-2])\\/([1-9]|0[1-9]|[12][0-9]|3[01])\\/([0-9][0-9][0-9][0-9])\")\n",
    "\n",
    "def check_date(text):\n",
    "    return len(list(date.generate(text))) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f97ab107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_date('1/14/23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7bb5dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_date('02/07/2024')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3072ee94-f244-4747-bb74-3c37a9aaa8e6",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c05deeed-16fd-491c-ad9c-6e8f4a0e4fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                   [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m10 passed\u001b[0m\u001b[32m in 0.08s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "init_test()\n",
    "\n",
    "YES_DATES = ('2/3/2023', '12/14/1999', '11/4/2020', '02/03/2000')\n",
    "NO_DATES = (' 2/3/2023', '22/1/2023', '5/89/1874', '2/1/20230', '2/1/2023/', '2/1/1')\n",
    "\n",
    "@pytest.mark.parametrize('text,isDate', [(x,True) for x in YES_DATES] + \n",
    "                                        [(x,False) for x in NO_DATES])\n",
    "def test_date(text, isDate):\n",
    "    assert check_date(text) == isDate\n",
    "\n",
    "run_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a0be4f-1be8-4cfd-8caa-a82ac8c137a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Numbers\n",
    "\n",
    "Transducer that maps the integers 1â€“99 to English (e.g. \"12\" to \"twelve\", \"46\" to \"forty six\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93088b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_defs = dict()\n",
    "\n",
    "# one digit numbers\n",
    "number_defs['s1'] = FST.re(r\"1:(one) | 2:(two) | 3:(three) | 4:(four) | 5:(five) | 6:(six) | 7:(seven) | 8:(eight) | 9:(nine)\", number_defs)\n",
    "\n",
    "# irregular teens\n",
    "number_defs['s2'] = FST.re(r\"(10):(ten) | (11):(eleven) | (12):(twelve) | (13):(thirteen) | (18):(eighteen)\", number_defs)\n",
    "\n",
    "# regular teens\n",
    "number_defs['s3'] = FST.re(r\"(1[4-7,9]) @ (1:'' $s1 '':(teen))\", number_defs)\n",
    "\n",
    "# tens\n",
    "number_defs['s4'] = FST.re(r\"2:(twenty) | 3:(thirty) | 4:(forty) | 5:(fifty) | 6:(sixty) | 7:(seventy) | 8:(eighty) | 9:(ninety)\", number_defs)\n",
    "\n",
    "# double digit numbers\n",
    "number_defs['s5'] = FST.re(r\"([2-9]0) @ ($s4 '':'' 0:'')\", number_defs)\n",
    "number_defs['s6'] = FST.re(r\"([2-9][1-9]) @ ($s4 '':' ' ([1-9] @ $s1))\", number_defs)\n",
    "\n",
    "number = FST.re(r\"$s1 | $s2 | $s3 | $s5 | $s6\", number_defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d86094f-1dbd-4fd3-a9c0-17726ba4ffba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['six']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(number.generate('6'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04a7501a-fca7-41c6-8ff3-d9527ed1da98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['16']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(number.analyze('sixteen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deac63e-0e1e-4b62-85de-12ecd061cdc5",
   "metadata": {},
   "source": [
    "Check your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfaba6ff-c896-4d86-a515-bfb35bc7c08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m [ 92%]\n",
      "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                      [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m99 passed\u001b[0m\u001b[32m in 0.83s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "init_test()\n",
    "\n",
    "PAIRS = [('1','one'),('2','two'),('3','three'),('4','four'),('5','five'),\n",
    "    ('6','six'),('7','seven'),('8','eight'),('9','nine'),('10','ten'),\n",
    "    ('11','eleven'),('12','twelve'),('13','thirteen'),('14','fourteen'),\n",
    "    ('15','fiveteen'),('16','sixteen'),('17','seventeen'),('18','eighteen'),\n",
    "    ('19','nineteen'),('20','twenty'),('21','twenty one'),('22','twenty two'),\n",
    "    ('23','twenty three'),('24','twenty four'),('25','twenty five'),\n",
    "    ('26','twenty six'),('27','twenty seven'),('28','twenty eight'),\n",
    "    ('29','twenty nine'),('30','thirty'),('31','thirty one'),('32','thirty two'),\n",
    "    ('33','thirty three'),('34','thirty four'),('35','thirty five'),\n",
    "    ('36','thirty six'),('37','thirty seven'),('38','thirty eight'),\n",
    "    ('39','thirty nine'),('40','forty'),('41','forty one'),('42','forty two'),\n",
    "    ('43','forty three'),('44','forty four'),('45','forty five'),\n",
    "    ('46','forty six'),('47','forty seven'),('48','forty eight'),\n",
    "    ('49','forty nine'),('50','fifty'),('51','fifty one'),('52','fifty two'),\n",
    "    ('53','fifty three'),('54','fifty four'),('55','fifty five'),\n",
    "    ('56','fifty six'),('57','fifty seven'),('58','fifty eight'),\n",
    "    ('59','fifty nine'),('60','sixty'),('61','sixty one'),('62','sixty two'),\n",
    "    ('63','sixty three'),('64','sixty four'),('65','sixty five'),\n",
    "    ('66','sixty six'),('67','sixty seven'),('68','sixty eight'),\n",
    "    ('69','sixty nine'),('70','seventy'),('71','seventy one'),\n",
    "    ('72','seventy two'),('73','seventy three'),('74','seventy four'),\n",
    "    ('75','seventy five'),('76','seventy six'),('77','seventy seven'),\n",
    "    ('78','seventy eight'),('79','seventy nine'),('80','eighty'),\n",
    "    ('81','eighty one'),('82','eighty two'),('83','eighty three'),\n",
    "    ('84','eighty four'),('85','eighty five'),('86','eighty six'),\n",
    "    ('87','eighty seven'),('88','eighty eight'),('89','eighty nine'),\n",
    "    ('90','ninety'),('91','ninety one'),('92','ninety two'),\n",
    "    ('93','ninety three'),('94','ninety four'),('95','ninety five'),\n",
    "    ('96','ninety six'),('97','ninety seven'),('98','ninety eight'),\n",
    "    ('99','ninety nine')]\n",
    "\n",
    "@pytest.mark.parametrize('digits,text', PAIRS)\n",
    "def test_number(digits,text):\n",
    "\n",
    "    assert list(number.generate(digits)) == [text]\n",
    "    assert list(number.analyze(text)) == [digits]\n",
    "\n",
    "run_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a07dde-3452-4467-988a-f5c6519afb74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "The following cell defines a regular expression for a simple tokenizer.\n",
    "\n",
    "It divides tokens up at spaces with some exceptions:\n",
    "- punctuation marks `,!?` are tokens by themselves.\n",
    "- the punctuation marks ` `` `  and `''` are single tokens\n",
    "- the contractions `n't`, `'ve`, `'ll`, `'re`, and `'s` are seperate tokens\n",
    "- numbers are separate tokens, where:\n",
    "    - a number may start with `$` or end with `%`\n",
    "    - a number may start with or contain a comma, but may not end with one\n",
    "\n",
    "**Note**: English tokenizers also usually have to worry about periods (`.`), which can be used to mark abbreviations, as decimal points, or to end a sentence (among other things). Unfortunately, pyfoma has some weird bugs in the way in handles periods, so we'll just ignore them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54d69451-76d9-499c-bf6a-d4848d07b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_patterns = {}\n",
    "\n",
    "# insert spaces before and after punctuation, including left and right double apostrophes\n",
    "tok_patterns['punct'] = FST.re(r\"$^rewrite('':' ' ([!?-] | ``) '':' ')\") \n",
    "tok_patterns['punct1'] = FST.re(r\"$^rewrite('':' ' [,] '':'')\") \n",
    "\n",
    "# insert space before contractions including n't, 've, 'll, 're, 's\n",
    "tok_patterns['contract'] = FST.re(r\"$^rewrite('':' ' (n\\'t|\\'ll|\\'re|\\'s) '':' ')\")\n",
    "tok_patterns['contract1'] = FST.re(r\"$^rewrite('':' ' (\\'ve) '':'')\")\n",
    "\n",
    "# handle numbers, which may start with $ or end with %, and may contain but not end with a comma\n",
    "tok_patterns['numbers'] = FST.re(r\"$^rewrite(' ':'' [$]?([,][0-9]+)[%]? '':'')\") \n",
    "\n",
    "# combine patterns\n",
    "tokenizer = FST.re(\"$punct @ $punct1 @ $contract @ $contract1 @ $numbers\", tok_patterns)\n",
    "\n",
    "def tokenize(s):\n",
    "    s = list(tokenizer.generate(s))\n",
    "    if len(s) == 1:\n",
    "        return s[0].split()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "252285dd-1e78-429f-b77b-abc05af5a4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do', \"n't\", 'you', 'love', 'transducers', '?']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"Don't you love transducers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "511418bf-c627-411d-a544-b0d0760997a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                                                   [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m_____________________ test_tokenizer[The word 'very' is very over-used-toks5] ______________________\u001b[0m\n",
      "\n",
      "text = \"The word 'very' is very over-used\", toks = ['The', 'word', \"'\", 'very', \"'\", 'is', ...]\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m'\u001b[39;49;00m\u001b[33mtext,toks\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, TEST_EXAMPLES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tokenizer\u001b[39;49;00m(text, toks):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m tokenize(text) == toks\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert ['The', 'word..., 'over', ...] == ['The', 'word...'\", 'is', ...]\u001b[0m\n",
      "\u001b[1m\u001b[31mE         At index 2 diff: \"'very'\" != \"'\"\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Right contains 2 more items, first extra item: '-'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         Use -v to get more diff\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/zp/_d_rstf11flfs_c9y4mt102w0000gn/T/ipykernel_45918/2713797998.py\u001b[0m:24: AssertionError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_288673f67fd240cc8873de71bc6a1a83.py::\u001b[1mtest_tokenizer[The word 'very' is very over-used-toks5]\u001b[0m - assert ['The', 'word..., 'over', ...] == ['The', 'word...'\", 'is', ...]\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m9 passed\u001b[0m\u001b[31m in 0.32s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "init_test()\n",
    "\n",
    "TEST_EXAMPLES = (\n",
    "    ('This is a test!', ['This','is','a','test','!']),\n",
    "    ('Is this a test?', ['Is','this','a','test','?']),\n",
    "    (\"I don't think this is a test\", ['I', 'do', \"n't\", 'think', 'this', 'is', 'a', 'test']),\n",
    "    (\"Thá»§y phi cÆ¡ cá»§a tÃ´i lÃ  Ä‘áº§y Ä‘á»§ cá»§a lÆ°Æ¡n\", \n",
    "        ['Thá»§y', 'phi', 'cÆ¡', 'cá»§a', 'tÃ´i', 'lÃ ', 'Ä‘áº§y', 'Ä‘á»§', 'cá»§a', 'lÆ°Æ¡n']),\n",
    "    (\"Is it legal to shout ``Fire!'' in a crowded theater?\", \n",
    "        ['Is', 'it', 'legal', 'to', 'shout', \"``\", 'Fire', '!', \"''\", 'in', 'a', 'crowded', 'theater','?']),\n",
    "    (\"The word 'very' is very over-used\", \n",
    "        ['The', 'word', \"'\", 'very', \"'\", 'is', 'very', 'over', '-', 'used']),\n",
    "    (\"I don't think we'll've been there yet\", \n",
    "        ['I', 'do', \"n't\", 'think', 'we', \"'ll\", \"'ve\", 'been', 'there', 'yet']),\n",
    "    (\"Give me 12 apples, please\", ['Give', 'me', '12', 'apples', ',', 'please']),    \n",
    "    (\"A 20% tip on a $30 tab is 6 dollars\", \n",
    "        ['A', '20%', 'tip', 'on', 'a', '$30', 'tab', 'is', '6', 'dollars']),\n",
    "    (\"They're going to pay us 10% of $120,000 by Jun 4, 2021\",\n",
    "        ['They', \"'re\", 'going', 'to', 'pay', 'us', '10%', 'of', '$120,000', 'by', 'Jun', '4', ',', '2021']),     \n",
    ")\n",
    "\n",
    "@pytest.mark.parametrize('text,toks', TEST_EXAMPLES)\n",
    "def test_tokenizer(text, toks):\n",
    "    assert tokenize(text) == toks\n",
    "\n",
    "run_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
